{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cours 05 : Données non structurées & programmation fonctionnelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troisième partie : dask "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask se présente comme une surcouche à toolz/cytoolz et numpy.\n",
    "Il n'offre pas de nouvelles fonctionnalité, mais vous permet de paralléliser les fonctions existantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cytoolz as ct # import groupby, valmap, compose\n",
    "import cytoolz.curried as ctc ## pipe, map, filter, get\n",
    "import sqlite3\n",
    "import pprint\n",
    "try:\n",
    "    import ujson as json\n",
    "except:\n",
    "    import json\n",
    "\n",
    "conn_sqlite = sqlite3.connect(\"twitter_for_network_100000.db\")\n",
    "cursor_sqlite = conn_sqlite.cursor()\n",
    "conn_sqlite_f = sqlite3.connect(\"twitter_for_network_full.db\")\n",
    "cursor_sqlite_f = conn_sqlite_f.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, psutil, gc\n",
    "import resource\n",
    "\n",
    "def memory_usage_psutil():\n",
    "    gc.collect()\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem = process.memory_info()[0] / float(2 ** 20)\n",
    "\n",
    "    print( \"Memory used : %i MB\" % mem )\n",
    "    print( \"Max memory usage : %i MB\" % (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss//1024) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On extrait les données de la base et on les écrit dans un fichiers plat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cursor_sqlite_f.execute(\"select content FROM tw_users\")\n",
    "\n",
    "with open(\"tw_users_all.json\", 'w') as f:\n",
    "    ct.count( ct.map( f.write, ct.interpose( \"\\n\", ct.pluck(0, cursor_sqlite_f ) ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sépare ce fichier plat en 50 morceaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.check_call( [\"split\", \"-d\", \"-n\", \"l/50\", \"--additional-suffix\", \".json\", \"tw_users_all.json\", \"tw_users_split_\" ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob \n",
    "for it_file in glob.glob(\"tw_users_split_*.json\"):\n",
    "    subprocess.check_call( [\"mv\", it_file, \"tw_users_splits\" ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée un objet de type dask.bag pour gérer l'ensemble de ces fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.bag as dbag\n",
    "try:\n",
    "    import ujson as json\n",
    "except:\n",
    "    import json\n",
    "from operator import add\n",
    "\n",
    "a = dbag.from_filenames('tw_users_splits/tw_users_split*.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va ensuite pouvoir utiliser la syntaxe cytoolz sur l'objet dbag.  \n",
    "dask va alors automatiquement gérer la parallélisation sur les différents fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 14 s per loop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1225448091"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.map(json.loads).pluck(\"followers_count\").fold(add).compute()\n",
    "%timeit -n1 b = a.map(json.loads).pluck(\"followers_count\").fold(add).compute()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention à l'état de votre mémoire quand vous utilisez dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "useless = [it**2 for it in range(25000000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del useless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la même façon, dask vous permet de paralléliser efficacement des opérations effectuées avec numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import dask.array as da\n",
    "\n",
    "big_random = da.random.normal( 1000, 20000, size = (50000,50000), chunks = (1000,1000) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000.312179118327"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_random.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous avez des données obtenues de façon classique sur numpy (ici générées de façon aléatoires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "for it in range(100):\n",
    "    a = np.random.random(size=(5000,5000))\n",
    "    h5f = h5py.File('data_{0:02d}.h5'.format(it), 'w')\n",
    "    h5f.create_dataset('dataset_1', data=a)\n",
    "    h5f.close()\n",
    "    if it % 5 == 0:\n",
    "        print(it)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask gère de façon transparente cet ensemble de matrice comme une seule matrice de dimension supérieure.\n",
    "\n",
    "Ci-dessous x est une matrice de taille 100 X 5000 X 5000, ce qui ne tiendrait absolument pas en mémoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5000, 5000)\n",
      "15.2774704128\n",
      "1250004675.56\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "dsets = [h5py.File(fn)['dataset_1'] for fn in sorted(glob('data_numpy/data_*.h5'))]\n",
    "arrays = [da.from_array(dset, chunks=(1000, 1000)) for dset in dsets]\n",
    "x = da.stack(arrays, axis=0)\n",
    "print(x.shape)\n",
    "print( x[:,0,0].sum().compute() )\n",
    "print( x[:,:,:].sum().compute() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used : 362 MB\n",
      "Max memory usage : 1651 MB\n"
     ]
    }
   ],
   "source": [
    "memory_usage_psutil()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
